# Deeply-Recursive Convolutional Network for Image Super-Resolution

Author: Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee
Research Institute: SNU
Reviewed: No

This paper introduces deeply-recursive convolutional network (DRCN), a SR network utilizing recursive-supervision and skip-connection

# Summary

### Problem:

Receptive field of a convolutional network determines amount of contextual information that can be used infer high-freq components

Widening receptive fields through increasing network depths include two approaches (with drawbacks)

- Use of conv layer with filter size larger than 1x1 (more parameters)
- Use of pool layer to reduce intermediate representation dimension (discard some pixel-wise information)

Use of pool layer is not appropriate for SR tasks as image details are important

Use of conv layers introduces more parameters which leads to two problems:

- Likely overfitting
- Model becomes too huge to be stored and received

To solve these issues, deeply-recursive convolutional network (DRCN) is proposed

Same convolutional layer can be repeated without increasing # of parameters

Yet DRCN optimized with stochastic gradient descent does not easily converge due to exploding/vanishing gradients

### Paper's **Solution:**

To ease the difficulty of training of DRCN

- All recursions are supervised
- Skip connection from input to reconstruction layer is used

### Proposed Methods

**Basic Model Architecture**

![Untitled](Deeply-Recursive%20Convolutional%20Network%20for%20Image%20S%206427da5df47f4ea7bb9497e8f2629ce8/Untitled.png)

The network is composed of :

Embedding - Inference - Reconstruction

1. Embedding network (f1)
    
    Input image → set of feature maps
    
    Learning is done in an end-to-end fashion with other subnetworks
    
    ![Untitled](Deeply-Recursive%20Convolutional%20Network%20for%20Image%20S%206427da5df47f4ea7bb9497e8f2629ce8/Untitled%201.png)
    
2. Inference network (f2)
    
    Set of feature maps → High-res feature maps
    
    Solves the task of super-resolution
    
    Use of recursion on conv filters larger than 1x1 to widen receptive field
    
    ![Untitled](Deeply-Recursive%20Convolutional%20Network%20for%20Image%20S%206427da5df47f4ea7bb9497e8f2629ce8/Untitled%202.png)
    
    ![Untitled](Deeply-Recursive%20Convolutional%20Network%20for%20Image%20S%206427da5df47f4ea7bb9497e8f2629ce8/Untitled%203.png)
    
3. Reconstruction network (f3)
    
    High-res feature maps → high-res image
    
    ![Untitled](Deeply-Recursive%20Convolutional%20Network%20for%20Image%20S%206427da5df47f4ea7bb9497e8f2629ce8/Untitled%204.png)
    

Problems in the model:

- Suffer from exploding/vanishing gradients
- Additionally, storing an exact copy of information through many recursion is not easy
- Finding optimal number of recursions is hard

**Advanced Model Architecture**

![Untitled](Deeply-Recursive%20Convolutional%20Network%20for%20Image%20S%206427da5df47f4ea7bb9497e8f2629ce8/Untitled%205.png)

**Recursive Supervision**

- Same representation can be used again and again during convolutions
- Outputs D predictions and all predictions are simultaneously supervised during training
- All D intermediate predicitions used to compute final output
- Backpropagation goes thorugh small number of layers as supervising signal goes deirectly from loss to early recursion

**Skip connection**

- Input image directly fed into reconstruction net whenever it is used during recursion
- Save network capacity
- Exact copy of input signal can be used during target prediction

**Training**

Use mean square error averaged over trainig set → favors PSNR

Recursive supervision means D+1 objectives to minimize

Hence loss function for intermediate outputs:

![Untitled](Deeply-Recursive%20Convolutional%20Network%20for%20Image%20S%206427da5df47f4ea7bb9497e8f2629ce8/Untitled%206.png)

And for final output:

![Untitled](Deeply-Recursive%20Convolutional%20Network%20for%20Image%20S%206427da5df47f4ea7bb9497e8f2629ce8/Untitled%207.png)

Total loss function is the following , where beta denotes multiplier of weight decay

![Untitled](Deeply-Recursive%20Convolutional%20Network%20for%20Image%20S%206427da5df47f4ea7bb9497e8f2629ce8/Untitled%208.png)

### Experiment and Evaluation

Tested on Set5, Set14, B100 and Urban100

16 recursions, momentum parameter 0.9 and weight decay of 0.0001, learning rate of 0.01

Recursion depth :

![Untitled](Deeply-Recursive%20Convolutional%20Network%20for%20Image%20S%206427da5df47f4ea7bb9497e8f2629ce8/Untitled%209.png)

Increasing recursion depth → larger image context and nonlinearities → performance boost

Best performances in all datasets

![Untitled](Deeply-Recursive%20Convolutional%20Network%20for%20Image%20S%206427da5df47f4ea7bb9497e8f2629ce8/Untitled%2010.png)

![Untitled](Deeply-Recursive%20Convolutional%20Network%20for%20Image%20S%206427da5df47f4ea7bb9497e8f2629ce8/Untitled%2011.png)

### Conclusion

- Presented SR method using a deeply-recursive convolutional network
- Efficiently reuse network parameters while exploiting a large image context
- Use of recursive supervision and skip connection’
- Demonstrate SotA results on benchmarks
